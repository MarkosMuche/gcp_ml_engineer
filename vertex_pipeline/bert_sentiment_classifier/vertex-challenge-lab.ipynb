{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19fe218-8272-4a78-95dc-b45c7944d26d",
   "metadata": {},
   "source": [
    "# Building and deploying machine learning solutions with Vertex AI: Challenge Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4908fb9b-2048-48fc-a42c-2fdf76aea51e",
   "metadata": {},
   "source": [
    "## Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386b37c-2ce1-4b1f-8c90-b83bda6075c8",
   "metadata": {},
   "source": [
    "* Train a TensorFlow model locally in a hosted [**Vertex Notebook**](https://cloud.google.com/vertex-ai/docs/general/notebooks?hl=sv).\n",
    "* Containerize your training code with [**Cloud Build**](https://cloud.google.com/build) and push it to [**Google Cloud Artifact Registry**](https://cloud.google.com/artifact-registry).\n",
    "* Define a pipeline using the [**Kubeflow Pipelines (KFP) V2 SDK**](https://www.kubeflow.org/docs/components/pipelines/sdk/v2/v2-compatibility) to train and deploy your model on [**Vertex Pipelines**](https://cloud.google.com/vertex-ai/docs/pipelines).\n",
    "* Query your model on a [**Vertex Endpoint**](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions) using online predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef6f12",
   "metadata": {},
   "source": [
    "**NOTE: Make sure you have installed the required packages for the lab as specified in the Task 2 > step 3 of the lab instructions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4480c8-710c-40dd-93c2-c51e67e59760",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0feaf4-9849-4636-b736-d3cd8a051579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add installed library dependencies to Python PATH variable.\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68df5dd-c456-4edd-8f58-71597f10c0ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve and set PROJECT_ID and REGION environment variables.\n",
    "# TODO: Fill in the PROJECT_ID and REGION provided in the lab manual.\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "PROJECT_ID = os.environ.get('PROJECT_ID')\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3912f9-6c12-439f-8613-cc60c286b3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Create a globally unique Google Cloud Storage bucket for artifact storage.\n",
    "GCS_BUCKET = \"gs://vetexai-bucket-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931ae91-3ba1-437a-9c37-187a41a3d227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil mb -l $REGION $GCS_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebbc2b-21ad-47f0-829f-9beba0deba9d",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf558fc-d0fc-4452-8281-7d7cd0cffe50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from official.nlp import optimization  \n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# # Install pydot and graphviz\n",
    "# !pip install pydot\n",
    "# !sudo apt install graphviz -y\n",
    "\n",
    "from trainer.model import load_datasets, build_text_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43371e-2c64-4a76-8698-fa768043dbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef491df4-c35f-4555-a6b6-96114c3d3c6e",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee70d2c-c0e3-4c75-9bc6-b42dad6c7267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "LOCAL_DATA_DIR = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f906a4-64a0-45ae-b376-757ef0f661fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DATASET_DIR = download_data(data_url=DATA_URL, local_data_dir=LOCAL_DATA_DIR)\n",
    "DATASET_DIR = 'aclImdb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a61fa-cf55-470f-9837-c783c4bcccf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to iteratively add data pipeline and model training hyperparameters.\n",
    "HPARAMS = {\n",
    "    # Set a random sampling seed to prevent data leakage in data splits from files.\n",
    "    \"seed\": 42,\n",
    "    # Number of training and inference examples.\n",
    "    \"batch-size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff05aa4-d299-4c80-a29a-43c6bc3ac152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_train_ds, raw_val_ds, raw_test_ds = load_datasets(DATASET_DIR, HPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee40c0-9e37-483c-98f5-dcdb467a2bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "CLASS_NAMES = raw_train_ds.class_names\n",
    "\n",
    "train_ds = raw_train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = raw_val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = raw_test_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5734e-d97c-484d-9f52-f6fb4e153ef0",
   "metadata": {},
   "source": [
    "Let's print a few example reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d794068-817c-4cb8-8e4c-c49860d0c92d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Review {i}: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({CLASS_NAMES[label]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22196658-1c30-49c7-8485-5d933fc8988e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HPARAMS.update({\n",
    "    # TF Hub BERT modules.\n",
    "    \"tfhub-bert-preprocessor\": \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "    \"tfhub-bert-encoder\": \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b634139-a0d1-41e7-be23-c6e580a4f0e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HPARAMS.update({\n",
    "    # Model training hyperparameters for fine tuning and regularization.\n",
    "    \"epochs\": 0,\n",
    "    \"initial-learning-rate\": 3e-5,\n",
    "    \"dropout\": 0.1 \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e415aeb-5ab2-42ac-904a-ae0649d45a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = HPARAMS['epochs']\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "n_train_steps = steps_per_epoch * epochs\n",
    "n_warmup_steps = int(0.1 * n_train_steps)    \n",
    "\n",
    "OPTIMIZER = optimization.create_optimizer(init_lr=HPARAMS['initial-learning-rate'],\n",
    "                                          num_train_steps=n_train_steps,\n",
    "                                          num_warmup_steps=n_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b72cc-9e1c-49c9-8a90-7b09b6108f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = build_text_classifier(HPARAMS, OPTIMIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8198df2-c15a-4f79-a154-83c941aba5f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize your fine-tuned BERT sentiment classifier.\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfeb71d-4e19-4759-8f5c-293c8c7cee1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_REVIEW = ['this is such an amazing movie!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2112f-9f04-470e-8636-38ea948cba9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BERT_RAW_RESULT = model(tf.constant(TEST_REVIEW))\n",
    "print(BERT_RAW_RESULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53dd9fa-d0a4-46ab-a123-3065b8fde7c8",
   "metadata": {},
   "source": [
    "### Train and evaluate your BERT sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f008fc-f696-4b71-9011-2a897d268795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HPARAMS.update({\n",
    "    # TODO: Save your BERT sentiment classifier locally in the form of <key>:<path to save the model>. \n",
    "    # Hint: You can use the key as 'model-dir' and save it to './bert-sentiment-classifier-local'.\n",
    "    \"model-dir\": './bert-sentiment-classifier-local'\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd08f3-988f-408a-b694-af4702de85ba",
   "metadata": {},
   "source": [
    "**Note:** training your model locally will take about 10-15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cab23-fbf0-44d2-9ee0-e2dd83390fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# history = train_evaluate(HPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91980420-8451-4869-b189-2b3693131ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# history_dict = history.history\n",
    "# print(history_dict.keys())\n",
    "\n",
    "# acc = history_dict['binary_accuracy']\n",
    "# val_acc = history_dict['val_binary_accuracy']\n",
    "# loss = history_dict['loss']\n",
    "# val_loss = history_dict['val_loss']\n",
    "\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "# fig = plt.figure(figsize=(10, 6))\n",
    "# fig.tight_layout()\n",
    "\n",
    "# plt.subplot(2, 1, 1)\n",
    "# # \"bo\" is for \"blue dot\"\n",
    "# plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# # b is for \"solid blue line\"\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# # plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc3fed-aa4c-40b2-8c44-19be21ba4689",
   "metadata": {},
   "source": [
    "## Containerize your model code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3905338-288b-4565-9abb-9053d7559315",
   "metadata": {},
   "source": [
    "Now that you trained and evaluated your model locally in a Vertex Notebook as part of an experimentation workflow, your next step is to train and deploy your model on Google Cloud's Vertex AI platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb61ec-cb3c-43bd-9d75-9d61ff52848e",
   "metadata": {},
   "source": [
    "To train your BERT classifier on Google Cloud, you will you will package your Python training scripts and write a Dockerfile that contains instructions on your ML model code, dependencies, and execution instructions. You will build your custom container with Cloud Build, whose instructions are specified in `cloudbuild.yaml` and publish your container to your Artifact Registry. This workflow gives you the opportunity to use the same container to run as part of a portable and scalable [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) workflow. \n",
    "\n",
    "\n",
    "You will walk through creating the following project structure for your ML mode code:\n",
    "```\n",
    "|--/bert-sentiment-classifier\n",
    "   |--/trainer\n",
    "      |--__init__.py\n",
    "      |--model.py\n",
    "      |--task.py\n",
    "   |--Dockerfile\n",
    "   |--cloudbuild.yaml\n",
    "   |--requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81292584-7a08-4c92-a9ba-e3dbc7005130",
   "metadata": {},
   "source": [
    "## Use Cloud Build to build and submit your model container to Google Cloud Artifact Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47400c-aa7c-4929-a584-f5067bd682eb",
   "metadata": {},
   "source": [
    "Next, you will use [Cloud Build](https://cloud.google.com/build) to build and upload your custom TensorFlow model container to [Google Cloud Artifact Registry](https://cloud.google.com/artifact-registry). \n",
    "\n",
    "Cloud Build brings reusability and automation to your ML experimentation by enabling you to reliably build, test, and deploy your ML model code as part of a CI/CD workflow. Artifact Registry provides a centralized repository for you to store, manage, and secure your ML container images. This will allow you to securely share your ML work with others and reproduce experiment results.\n",
    "\n",
    "**Note**: the initial build and submit step will take about 16 minutes but Cloud Build is able to take advantage of caching for faster subsequent builds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c0d02-200f-4cc3-bdfd-ba96d233ecc4",
   "metadata": {},
   "source": [
    "### 1. Create Artifact Registry for custom container images \n",
    "\n",
    "**NOTE:** For any help to create the Artifact Registry, you can refer this [Documentation](https://cloud.google.com/sdk/gcloud/reference/artifacts/repositories/create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9918475-f6dc-4fa3-8249-47976e68f529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ARTIFACT_REGISTRY=\"bert-sentiment-classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9566d-c6c4-48c8-b4f8-ad239e5ae349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: create a Docker Artifact Registry using the gcloud CLI. Note the required 'repository-format', 'location' and 'description' flags while creating the Artifact Registry.\n",
    "# Documentation link: https://cloud.google.com/sdk/gcloud/reference/artifacts/repositories/create\n",
    "# Create a Docker Artifact Registry using the gcloud CLI with correct parameters\n",
    "\n",
    "\n",
    "# !gcloud artifacts repositories create bert-sentiment-classifier \\\n",
    "#     --repository-format=docker \\\n",
    "#     --location={REGION} \\\n",
    "#     --description=\"Docker repository for BERT sentiment classifier\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900832e-de90-4ba1-ba7d-7973a1de9cc1",
   "metadata": {},
   "source": [
    "### 2. Create `cloudbuild.yaml` instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580619d-957c-409f-ac15-ccbc0bb79a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_NAME=\"bert-sentiment-classifier\"\n",
    "IMAGE_TAG=\"latest\"\n",
    "IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REGISTRY}/{IMAGE_NAME}:{IMAGE_TAG}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24790970-988e-4694-b8cd-a2d9d500c11b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cloudbuild_yaml = f\"\"\"steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: [ 'build', '-t', '{IMAGE_URI}', '.' ]\n",
    "images: \n",
    "- '{IMAGE_URI}'\"\"\"\n",
    "\n",
    "with open(f\"cloudbuild.yaml\", \"w\") as fp:\n",
    "    fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14b9c6-c120-482d-b5ab-c6a4c53bb205",
   "metadata": {},
   "source": [
    "### 3. Build and submit your container image to Artifact Registry using Cloud Build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e154b1-e584-496c-97b4-14795f80928b",
   "metadata": {},
   "source": [
    "**Note:** your custom model container will take about 16 minutes initially to build and submit to your Artifact Registry. Artifact Registry is able to take advantage of caching so subsequent builds take about 10 minutes. For any help to submit a build, you can refer this [**documentation**](https://cloud.google.com/sdk/gcloud/reference/builds/submit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf0093-d66e-49c1-8a55-047020735e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: use Cloud Build to build and submit your custom model container to your Artifact Registry.\n",
    "# Documentation link: https://cloud.google.com/sdk/gcloud/reference/builds/submit\n",
    "# Hint: make sure the config flag is pointed at `{MODEL_DIR}/cloudbuild.yaml` defined above and you include your model directory as {MODEL_DIR}. Also, add a timeout flag.\n",
    "\n",
    "# !gcloud builds submit . \\\n",
    "#     --config=./cloudbuild.yaml \\\n",
    "#     --timeout=900s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77355b83-577b-4831-9862-91e08e974256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "from my_pipeline import pipeline\n",
    "TEMPLATE_PATH = 'pipeline.json'\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=TEMPLATE_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cda30-4046-4d29-abdd-501c243f5eee",
   "metadata": {},
   "source": [
    "## Run the pipeline on Vertex Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be420d-9d1d-4e8e-a08a-658fdfd60eb0",
   "metadata": {},
   "source": [
    "The `PipelineJob` is configured below and triggered through the `run()` method.\n",
    "\n",
    "**Note:** This pipeline run will take around **30-40** minutes to train and deploy your model. Follow along with the execution using the URL from the job output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP=datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "DISPLAY_NAME = \"bert-sentiment-{}\".format(TIMESTAMP)\n",
    "GCS_BASE_OUTPUT_DIR= f\"{GCS_BUCKET}/bert_sentiment_classifier-{TIMESTAMP}\"\n",
    "SERVING_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276575d-c2ba-4d08-9a2a-b7583af27aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertex_pipelines_job = aiplatform.PipelineJob(\n",
    "    display_name=\"bert-sentiment-classification\",\n",
    "    template_path=TEMPLATE_PATH,\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"location\": REGION,\n",
    "        \"staging_bucket\": GCS_BUCKET,\n",
    "        \"display_name\": DISPLAY_NAME,        \n",
    "        \"container_uri\": IMAGE_URI,\n",
    "        \"model_serving_container_image_uri\": SERVING_IMAGE_URI,        \n",
    "        \"base_output_dir\": GCS_BASE_OUTPUT_DIR},\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0ab35e9-207c-49ea-8a27-6e6cddce8541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertex_pipelines_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a821a-a3bd-45bf-a9ea-aa18687218f6",
   "metadata": {},
   "source": [
    "## Query deployed model on Vertex Endpoint for online predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd8366-d362-4537-ab30-21c21fce6846",
   "metadata": {},
   "source": [
    "Finally, you will retrieve the `Endpoint` deployed by the pipeline and use it to query your model for online predictions.\n",
    "\n",
    "Configure the `Endpoint()` function below with the following parameters:\n",
    "\n",
    "*  `endpoint_name`: A fully-qualified endpoint resource name or endpoint ID. Example: \"projects/123/locations/us-central1/endpoints/456\" or \"456\" when project and location are initialized or passed.\n",
    "*  `project_id`: GCP project.\n",
    "*  `location`: GCP region.\n",
    "\n",
    "Call `predict()` to return a prediction for a test review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80748b-8907-4ad6-8adb-d4c394752257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your deployed Endpoint name from your pipeline.\n",
    "ENDPOINT_NAME = aiplatform.Endpoint.list()[0].name\n",
    "ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c989d-1026-4f57-8dac-dafad01145a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Generate online predictions using your Vertex Endpoint. \n",
    "#Hint: You need to add the following variables: endpoint_name, project, location, with their required values.\n",
    "\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    location = REGION,\n",
    "    project=PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97759f45-e060-44ce-87fc-4d34c4b8cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: write a movie review to test your model e.g. \"The Dark Knight is the best Batman movie!\"\n",
    "test_review = [\"This is fantastic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c008ce-90ad-4709-a24f-36b414c779e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use your Endpoint to return prediction for your 'test_review' using 'endpoint.predict()' method.\n",
    "prediction = endpoint.predict(test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54751a3e-7b2a-4ab8-b642-6533df27de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4c68e-a937-44f8-b64a-cbe9e607cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a sigmoid function to compress your model output between 0 and 1. For binary classification, a threshold of 0.5 is typically applied\n",
    "# so if the output is >= 0.5 then the predicted sentiment is \"Positive\" and < 0.5 is a \"Negative\" prediction.\n",
    "print(tf.sigmoid(prediction.predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344e3eb-0a0e-4271-b815-e792d8c95b66",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80557132-f0cf-4f4d-be5d-2c58453bc6b6",
   "metadata": {},
   "source": [
    "Congratulations! You walked through a full experimentation, containerization, and MLOps workflow on Vertex AI. First, you built, trained, and evaluated a BERT sentiment classifier model in a Vertex Notebook. You then packaged your model code into a Docker container to train on Google Cloud's Vertex AI. Lastly, you defined and ran a Kubeflow Pipeline on Vertex Pipelines that trained and deployed your model container to a Vertex Endpoint that you queried for online predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6570ed8-a1ae-41e0-8a0b-9b63ca972d85",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2c9ee-e982-4b8b-91c3-02f313896c6c",
   "metadata": {},
   "source": [
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
